{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('sigmund': pipenv)",
   "metadata": {
    "interpreter": {
     "hash": "3a5ecb16c9faf077c277bfdc9be29850d6e376b4e842362c3656fa7c096a98f0"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display \n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.path.join(os.getcwd(), \"data\", \"transcripts\")\n",
    "files = [os.path.join(root, f) for root, _, files in os.walk(folder) for f in files if f.endswith(\".docx\")]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.pipelinelib.querying import Parser, Queryable\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\", disable=[\"ner\", \"parser\"])\n",
    "parser = Parser(nlp=nlp, metadata_path=\"./data/transcripts/Kopie von Transkriptionspaare_Daten.xls\")\n",
    "\n",
    "parser.read_from_files(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(parser.frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelinelib.querying import Parser, Queryable\n",
    "from src.pipelinelib.text_body import TextBody\n",
    "\n",
    "queryable = Queryable.from_parser(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_df = queryable.execute(level=TextBody.DOCUMENT)\n",
    "display(document_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_df = queryable.execute(level=TextBody.PARAGRAPH)\n",
    "display(paragraph_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df = queryable.execute(level=TextBody.SENTENCE)\n",
    "display(sentence_df)\n",
    "\n",
    "queryable.execute(level=TextBody.SENTENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgs = queryable.by_couple_id(couple_id=27) \\\n",
    "    .is_depressed(d=False) \\\n",
    "    .execute(level=TextBody.DOCUMENT)\n",
    "display(pgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.sigmund.preprocessing.words import Tokenizer, Stemmer, Lemmatizer\n",
    "from src.sigmund.features.tfidf import FeatureTFIDF\n",
    "from src.pipelinelib.pipeline import Pipeline\n",
    "from src.sigmund.classification.naive_bayes import NaiveBayes\n",
    "from src.sigmund.classification.merger import FeatureMerger\n",
    "from src.sigmund.classification.linear_discriminant_analysis import LinearDiscriminantAnalysisClassifier\n",
    "from src.sigmund.features.liwc import Liwc\n",
    "from src.sigmund.features.vocabulary_size import VocabularySize\n",
    "from src.sigmund.classification.pca import PCAReduction\n",
    "from src.sigmund.extensions import *\n",
    "from src.sigmund.features.pos import PartOfSpeech\n",
    "from src.sigmund.features.basic_statistics import BasicStatistics\n",
    "\n",
    "pipeline = Pipeline(queryable=queryable)\n",
    "pipeline.add_components([Tokenizer(), Stemmer(), Lemmatizer()])\n",
    "pipeline.add_component(FeatureTFIDF(white_list=[\n",
    "    'ja', 'auch', 'wenn', 'also', 'werden', 'schon', 'wir', # high in depressed group\n",
    "    'und', 'haben', 'du', 'sehr'])), #'so', 'wirkl  ich', 'ich', 'gerne', 'weil']))\n",
    "pipeline.add_component(NaiveBayes(inputs=[TFIDF_DOCUMENT_MF], output=CLASSIFICATION_NAIVE_BAYES_TFIDF, voting=False))\n",
    "\n",
    "pipeline.add_component(Liwc(white_list=[\n",
    "    'Posemo', 'Past', 'Present', 'Future', 'Metaph',\n",
    "    'Death', 'Affect', 'Incl', 'Achieve'\n",
    "]))\n",
    "pipeline.add_component(NaiveBayes(inputs=[LIWC_DOCUMENT_MF], output=CLASSIFICATION_NAIVE_BAYES_LIWC, voting=False))\n",
    "\n",
    "pipeline.add_component(PartOfSpeech(white_list=[\"ADV\", \"PPER\", \"ADJD\", \"VAFIN\", \"KON\"]))\n",
    "pipeline.add_component(NaiveBayes(inputs=[POS_DOCUMENT_MF], output=CLASSIFICATION_NAIVE_BAYES_POS, voting=False))\n",
    "\n",
    "pipeline.add_component(NaiveBayes(inputs=[\n",
    "    CLASSIFICATION_NAIVE_BAYES_TFIDF, \n",
    "    CLASSIFICATION_NAIVE_BAYES_LIWC, \n",
    "    CLASSIFICATION_NAIVE_BAYES_POS,\n",
    "], output=CLASSIFICATION_NAIVE_BAYES_VOTING, voting=True))\n",
    "\n",
    "storage = pipeline.execute(visualise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}